{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Example: \n",
    "### AI, Machine Learning & Data Science \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Value for your Business?\n",
    "\n",
    "\n",
    "- By seeing acutal examples you'll be empowered to ask the right questions (and get fair help from consultants, startups, or data analytics companies)\n",
    "- This will help you make the correct decisions for your business\n",
    "\n",
    "# Demystify\n",
    "\n",
    "This is a real world example of how you'd solve a Machine Learning prediciton problem.\n",
    "\n",
    "**Common Machine Learning Use Cases in Companies:**\n",
    "- Discover churn risk of customers\n",
    "- Predict optimal price levels (investments / retail)\n",
    "- Predict future revenues\n",
    "- Build recommendation systems\n",
    "- Customer value scoring\n",
    "- Fraud detection\n",
    "- Customer insights (characteristics)\n",
    "- Predict sentiment of text / client feedback\n",
    "- Object detecton in images\n",
    "- etc etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python?\n",
    "\n",
    "Python is general purpose and can do Software development, Web development, AI. Python has experienced incredible growth over the last couple of years.\n",
    "\n",
    "<img src='https://zgab33vy595fw5zq-zippykid.netdna-ssl.com/wp-content/uploads/2017/09/growth_major_languages-1-1400x1200.png' width=400px></img>\n",
    "\n",
    "Source: https://stackoverflow.blog/2017/09/06/incredible-growth-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is free!\n",
    "\n",
    "The best software today is open source and it's also enterprise-ready. Anyone can download and use them for free (even for business purposes).\n",
    "\n",
    "**Examples of great, free AI libraries:**\n",
    "* Anaconda\n",
    "* Google's TensorFlow\n",
    "* Scikit-learn\n",
    "* Pandas\n",
    "* Keras\n",
    "* Matplotlib\n",
    "* SQL\n",
    "* Spark\n",
    "* Numpy\n",
    "\n",
    "## State-of-the-Art algorithms\n",
    "\n",
    "No matter what algorithm you want to use (Linear Regression, Random Forests, Neural Networks, or Deep Learning), **all of the latest methods are implemented optimized for Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "Python code can run on any computer. Therefore, you can scale your computations and utilize for example cloud resources to run big data jobs.\n",
    "\n",
    "**Great tools for Big Data:**\n",
    "- Spark\n",
    "- Databricks\n",
    "- Hadoop / MapReduce\n",
    "- Kafka\n",
    "- Amazon EC2\n",
    "- Amazon S3\n",
    "\n",
    "\n",
    "# Note on data collection\n",
    "\n",
    "- Collect all the data you can! (storage is cheap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Real world example of AI: Titanic Analysis\n",
    "\n",
    "Titanic notebook is open source. All of our material is online. Anyone can developt sophisticated AI programs and solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## The difficult part is never to implement the algorithm\n",
    "\n",
    "The hard part of a machine learning problem is to get data into the right format so you can solve the problem. We'll illustrate this below.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# __Titanic Survivor Analysis__\n",
    "\n",
    "\n",
    "**Sources:** \n",
    "* **Training + explanations**: https://www.kaggle.com/c/titanic\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding the connections between passanger information and survival rate\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\n",
    "\n",
    "### **Our task is to train a machine learning model on a data set consisting of 891 samples of people who were onboard of the Titanic. And then, be able to predict if the passengers survived or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bays\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from plot_distribution import plot_distribution\n",
    "plt.rcParams['figure.figsize'] = (9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "___\n",
    "## Part 2: Exploring the Data\n",
    "**Data descriptions**\n",
    "\n",
    "<img src=\"data/Titanic_Variable.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(13,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set?\n",
    "y_numbers = df['Survived'].map({0:'Deceased',1:'Survived'}).value_counts()\n",
    "\n",
    "y_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced data set, our classifiers have to outperform 62 % accuracy\n",
    "\n",
    "y_numbers[1] / y_numbers[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### __Interesting Fact:__ \n",
    "\n",
    "> Third Class passengers were the first to board, with First and Second Class passengers following up to an hour before departure. \n",
    "\n",
    "> Third Class passengers were inspected for ailments and physical impairments that might lead to their being refused entry to the United States, while First Class passengers were personally greeted by Captain Smith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of survival rate for the socioeconmic classes?\n",
    "\n",
    "df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True) \\\n",
    "                    .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "> #### __Brief Remarks Regarding the Data__\n",
    "\n",
    "> * `PassengerId` is a random number (incrementing index) and thus does not contain any valuable information. \n",
    "\n",
    "> * `Survived, Passenger Class, Age, Siblings Spouses, Parents Children` and `Fare` are numerical values (no need to transform them) -- but, we might want to group them (i.e. create categorical variables). \n",
    "\n",
    "> * `Sex, Embarked` are categorical features that we need to map to integer values. `Name, Ticket` and `Cabin` might also contain valuable information.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary data\n",
    "__Note:__ It is important to remove variables that convey information already captured by some other variable. Doing so removes the correlation, while also diminishing potential overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Ticket', 'Cabin', 'Fare' need to do it \n",
    "# for both test and training\n",
    "\n",
    "df = df.drop(['PassengerId','Ticket', 'Cabin','Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "____\n",
    "## Part 3: Transforming the data\n",
    "\n",
    "### 3.1 _The Title of the person can be used to predict survival_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List example titles in Name column\n",
    "df.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column called Title\n",
    "\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that our titles makes sense (by comparing to sex)\n",
    "\n",
    "pd.crosstab(df['Title'], df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map rare titles to one group\n",
    "\n",
    "df['Title'] = df['Title'].\\\n",
    "              replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr',\\\n",
    "             'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "df['Title'] = df['Title'].replace('Mlle', 'Miss') #Mademoiselle\n",
    "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Mme', 'Mrs') #Madame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have more logical (contemporary) titles, and fewer groups\n",
    "# See if we can get some insights\n",
    "\n",
    "df[['Title', 'Survived']].groupby(['Title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can plot the survival chance for each title\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Title\", data=df, order=[1,0])\n",
    "plt.xticks(range(2),['Survived','Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title dummy mapping: Map titles to binary dummy columns\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Title)\n",
    "df[binary_encoded.columns] = binary_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique variables for analysis (Title is generally bound to Name, so it's also dropped)\n",
    "df = df.drop(['Name', 'Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Gender column to binary (male = 0, female = 1) categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variable to numeric\n",
    "\n",
    "df['Sex'] = df['Sex']. \\\n",
    "    map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age = df.Age.fillna(df.Age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split age into bands and look at survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bands\n",
    "df['AgeBand'] = pd.cut(df['Age'], 5)\n",
    "df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False)\\\n",
    "                    .mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suvival probability against age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relative survival rate distributions against Age of passangers\n",
    "# subsetted by the gender\n",
    "\n",
    "plot_distribution( df , var = 'Age' , target = 'Survived' ,\\\n",
    "                  row = 'Sex' )\n",
    "\n",
    "# Recall: {'male': 0, 'female': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Age column to\n",
    "# map Age ranges (AgeBands) to ordinal integer numbers\n",
    "\n",
    "df.loc[ df['Age'] <= 16, 'Age'] = 0\n",
    "df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n",
    "df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n",
    "df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n",
    "df.loc[ df['Age'] > 64, 'Age']=4\n",
    "df = df.drop(['AgeBand'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Note we could just run \n",
    "# df['Age'] = pd.cut(df['Age'], 5,labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Party Size\n",
    "\n",
    "How did the number of people the person traveled with impact the chance of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp = Number of Sibling / Spouses\n",
    "# Parch = Parents / Children\n",
    "\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Survival chance against FamilySize\n",
    "df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True) \\\n",
    "                                .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"FamilySize\", data=df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable if the person was alone or not\n",
    "\n",
    "df['IsAlone'] = 0\n",
    "df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the binary IsAlone feature for further analysis\n",
    "\n",
    "df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new features based on intuitive combinations\n",
    "# Here is an example when we say that the age times socioclass is a determinant factor\n",
    "\n",
    "df['Age*Class'] = df.Age.values * df.Pclass.values\n",
    "\n",
    "df.loc[:, ['Age*Class', 'Age', 'Pclass']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port the person embarked from\n",
    "Let's see how that influences chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"data/images/titanic_voyage_map.png\">\n",
    ">___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN 'Embarked' Values in the dfs\n",
    "freq_port = df['Embarked'].dropna().mode()[0]\n",
    "df['Embarked'] = df['Embarked'].fillna(freq_port)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Embarked\", data=df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True) \\\n",
    "                    .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical dummy variables for Embarked values\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Embarked)\n",
    "df[binary_encoded.columns] = binary_encoded\n",
    "df.drop('Embarked', axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished -- Preprocessing Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are approximately on the same scale\n",
    "# no need for feature engineering / normalization\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: View the correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrelated features are generally more powerful predictors\n",
    "\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(df.corr().round(2)\\\n",
    "            ,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \\\n",
    "            linecolor='white', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "___\n",
    "### Machine Learning, Prediction and Artifical Intelligence\n",
    "Now we will use Machine Learning algorithms in order to predict if the person survived. \n",
    "\n",
    "**We will choose the best model from:**\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN) \n",
    "3. Support Vector Machines (SVM)\n",
    "4. Perceptron\n",
    "5. XGBoost\n",
    "6. Random Forest\n",
    "7. Neural Network (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Survived\", axis=1) # Training & Validation data\n",
    "Y = df[\"Survived\"]              # Response / Target Variable\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set so that we validate on 20% of the data\n",
    "# Note that our algorithms will never have seen the validation \n",
    "\n",
    "np.random.seed(1337) # set random seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = \\\n",
    "                train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print('Training Samples:', X_train.shape, Y_train.shape)\n",
    "print('Validation Samples:', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> ## General ML workflow\n",
    "> 1. Create Model Object\n",
    "> 2. Train the Model\n",
    "> 3. Predict on _unseen_ data\n",
    "> 4. Evaluate accuracy.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Prediciton Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()           # create\n",
    "logreg.fit(X_train, Y_train)            # train\n",
    "acc_log_2 = logreg.score(X_val, Y_val)  # predict & evaluate\n",
    "\n",
    "print('Logistic Regression accuracy:',\\\n",
    "      str(round(acc_log_2*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)                  # instantiate\n",
    "knn.fit(X_train, Y_train)                                    # fit\n",
    "acc_knn = knn.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('K-Nearest Neighbors labeling accuracy:', str(round(acc_knn*100,2)),'%')                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines Classifier (non-linear kernel)\n",
    "svc = SVC()                                                  # instantiate\n",
    "svc.fit(X_train, Y_train)                                    # fit\n",
    "acc_svc = svc.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('Support Vector Machines labeling accuracy:', str(round(acc_svc*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()                                    # instantiate \n",
    "perceptron.fit(X_train, Y_train)                             # fit\n",
    "acc_perceptron = perceptron.score(X_val, Y_val)              # predict + evalaute\n",
    "\n",
    "print('Perceptron labeling accuracy:', str(round(acc_perceptron*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(n_estimators=1000)             # instantiate\n",
    "gradboost.fit(X_train, Y_train)                              # fit\n",
    "acc_xgboost = gradboost.score(X_val, Y_val)                  # predict + evalute\n",
    "\n",
    "print('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=500)   # instantiate\n",
    "random_forest.fit(X_train, Y_train)                         # fit\n",
    "acc_rf = random_forest.score(X_val, Y_val)                  # predict + evaluate\n",
    "\n",
    "print('Random Forest labeling accuracy:', str(round(acc_rf*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Neural Networks (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Dense(units=300, activation='relu', input_shape=(13,) ))\n",
    "model.add( Dense(units=100, activation='relu'))\n",
    "model.add( Dense(units=50, activation='relu'))\n",
    "model.add( Dense(units=1, activation='sigmoid') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs = 50, batch_size= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "print('Neural Network accuracy:',str(round(model.evaluate(X_val, Y_val, batch_size=50,verbose=False)[1]*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance scores in the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at importnace of features for random forest\n",
    "\n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print ('Training accuracy Random Forest:',model.score( X , y ))\n",
    "\n",
    "plot_model_var_imp(random_forest, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "___\n",
    "\n",
    "## Appendix I:\n",
    "#### Why are our models maxing out at around 80%?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __John Jacob Astor__\n",
    "\n",
    "<img src= \"data/images/john-jacob-astor.jpg\"> \n",
    "\n",
    "John Jacob Astor perished in the disaster even though our model predicted he would survive. Astor was the wealthiest person on the Titanic -- his ticket fare was valued at over 35,000 USD in 2016 -- it seems likely that he would have been among of the approximatelly 35 percent of men in first class to survive. However, this was not the case: although his pregnant wife survived, John Jacob Astorâ€™s body was recovered a week later, along with a gold watch, a diamond ring with three stones, and no less than 92,481 USD (2016 value) in cash.\n",
    "\n",
    "<br >\n",
    "\n",
    "\n",
    "#### __Olaus Jorgensen Abelseth__\n",
    "\n",
    "<img src= \"data/images/olaus-jorgensen-abelseth.jpg\">\n",
    "\n",
    "Avelseth was a 25-year-old Norwegian sailor, a man in 3rd class, and not expected to survive by classifier. However, once the ship sank, he survived by swimming for 20 minutes in the frigid North Atlantic water before joining other survivors on a waterlogged collapsible boat.\n",
    "\n",
    "Abelseth got married three years later, settled down as a farmer in North Dakota, had 4 kids, and died in 1980 at the age of 94.\n",
    "\n",
    "<br >\n",
    "\n",
    "### __Key Takeaway__ \n",
    "\n",
    "As engineers and business professionals we are trained to answer the question 'what could we do to improve on an 80 percent average'. These data points represent real people. Each time our model was wrong we should be glad -- in such misclasifications we will likely find incredible stories of human nature and courage triumphing over extremely difficult odds. \n",
    "\n",
    "__It is important to never lose sight of the human element when analyzing data that deals with people.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "___\n",
    "## Appendix II: Resources and references to material we won't cover in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Gradient Boosting:** http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "> * **Jupyter Notebook (tutorial):** https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "\n",
    "> * **K-Nearest Neighbors (KNN):** https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26\n",
    "\n",
    "> * **Logistic Regression:** https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\n",
    "\n",
    "> * **Naive Bayes:** http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "> * **Perceptron:** http://aass.oru.se/~lilien/ml/seminars/2007_02_01b-Janecek-Perceptron.pdf\n",
    "\n",
    "> * **Random Forest:** https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "\n",
    "> * **Support Vector Machines (SVM):** https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "\n",
    "\n",
    "<br>\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
